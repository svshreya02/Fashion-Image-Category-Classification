{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm  # Import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.46.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lable Creation and Image Loading and Split training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/jovyan/.cache/torch/checkpoints/resnet50-19c8e357.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2521ce0a97cf4d89a31743e9b9dd840f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.labels_df = pd.read_csv(csv_file, header=None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels_df[1] = self.label_encoder.fit_transform(self.labels_df[1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.labels_df.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        label = torch.tensor(self.labels_df.iloc[idx, 1], dtype=torch.long)  # Convert label to tensor\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((60, 80)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to RGB\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create custom dataset\n",
    "dataset = CustomDataset(csv_file='/home/jovyan/efs/users/Shreya_Sivakumar/TASK_1/data/labels.txt', root_dir='/home/jovyan/efs/users/Shreya_Sivakumar/TASK_1/data/images', transform=transform)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the last layer for the number of classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(dataset.label_encoder.classes_))\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Evaluating and Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 129.02946296078153\n",
      "Accuracy on test set: 98.74001574980312%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "    for inputs, labels in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix({'Loss': running_loss / len(train_loader)})  # Update progress bar with current loss\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), '/home/jovyan/efs/users/Shreya_Sivakumar/TASK_1/models/torch.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the model and defining Functions to predict for query image and extract colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2c3d6000e22c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the trained model for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnum_ftrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ftrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jovyan/efs/users/Shreya_Sivakumar/TASK_1/models/torch.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the trained model for prediction\n",
    "model = torchvision.models.resnet50(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(dataset.label_encoder.classes_))\n",
    "model.load_state_dict(torch.load('/home/jovyan/efs/users/Shreya_Sivakumar/TASK_1/models/torch.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((60, 80)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to RGB\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define function to predict category of a query image\n",
    "def predict_category(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_label = dataset.label_encoder.inverse_transform([predicted.item()])  # Convert to list\n",
    "    return predicted_label[0]\n",
    "\n",
    "def extract_colors(image_path, num_colors=5):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert image from BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Flatten the image to a 2D array of pixels\n",
    "    pixels = image.reshape(-1, 3)\n",
    "    \n",
    "    # Apply K-means clustering to find dominant colors\n",
    "    kmeans = KMeans(n_clusters=num_colors)\n",
    "    kmeans.fit(pixels)\n",
    "    \n",
    "    # Get the dominant colors\n",
    "    colors = kmeans.cluster_centers_\n",
    "    \n",
    "    return colors.astype(int)\n",
    "\n",
    "def display_colors(colors):\n",
    "    # Create a square image with the extracted colors\n",
    "    color_img = np.zeros((100, len(colors)*100, 3), dtype=np.uint8)\n",
    "    for i, color in enumerate(colors):\n",
    "        color_img[:, i*100:(i+1)*100] = color\n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(color_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Class and Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category for query image: Footwear\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABRCAYAAABxPXV4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAABiUlEQVR4nO3aTUoDQRhF0UpaSDYlQXSgy1TwZ4VtIIF2BTq8JeScaU3e6PINardt2wCgsZ89AOCWiC5ASHQBQqILEBJdgJDoAoTu/npc19V/sjHGun6Ph9PTOJ/Ps6dMdzgcxsfX5zgej7OnTHe5XMbb6/u4Xq+zp0y3LPtxOt2PZVlmT/kXnl8ed7+9uXQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILENpt2zZ7A8DNcOkChEQXICS6ACHRBQiJLkBIdAFCP+TYHJ03Gd8fAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "query_image_path = \"/home/jovyan/efs/users/Shreya_Sivakumar/TASK_1/data/images2.jpeg\"\n",
    "predicted_category = predict_category(query_image_path)\n",
    "print(f\"Predicted category for query image: {predicted_category}\")\n",
    "colors = extract_colors(query_image_path)\n",
    "display_colors(colors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
